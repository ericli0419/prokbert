{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG59DW3yUyVJ"
   },
   "source": [
    "# Training Helper Utilities\n",
    "To ease the process of training and evaluation multiple models, we have implemented two helper classes `TrainingHelperM` and `TrainingHelperD`, alongside a few helper functions like `get_finetuned_model_name`. These tools help us keep a tidy and safe code.\n",
    "\n",
    "## `TrainingHelperM`\n",
    "This class stores metadata regarding the model we are training. Its attributes are the following:\n",
    "- `HF_model_name`: This is the Hugging Face identifier of the base model we are using.\n",
    "- `huggingface_prefix`: The prefix string. Name of the developer team.\n",
    "- `base_model_name`: This is only the name of the model without prefixes.\n",
    "- `epochs`: Number of training epochs.\n",
    "- `learning_rate`: Learning rate used to train the model.\n",
    "- `seq_len`: Max length of the training sequences.\n",
    "- `batch_size`: Training batch size. This is auto inferred for known model (see below at `__init__()`).\n",
    "- `gradient_accumulation_steps`: Gradient accumulation steps. Auto-inferred too.\n",
    "\n",
    "### Methods:\n",
    "`TrainingHelperM` has a few methods to further help the programmer:\n",
    "- `from_json(path)`: Loads the model from a JSON file (more below).\n",
    "- `get_default_model()`: This method returns a Hugging Face AutoModelForSequenceClassification instantiated from the name stored in the helper.\n",
    "- `get_tokenizer()`: Returns the matching tokenizer to the model.\n",
    "- `get_tokenizer_function()`: Returns a tokenizer function which can be used by `dataset.map()` for pre-tokenization and other tasks.\n",
    "- `initialize_from_environment()`: Instantiates the model helper from environmental variables (more below).\n",
    "- `initialize_from_finetuned_name(name)`: Instantiates the model helper from a finetuned name (more below).\n",
    "- `to_json(path)`: The helper can save its metadata to a JSON file.\n",
    "\n",
    "### Initialization of the class\n",
    "There are four ways to initialize a `TrainingHelperM` class. First off we can do so by calling it's `__init__()` method. Secondly we can use the `initialize_from_environment` class method, to pull the metadata from environment variables. This is extremely helpful for SLURM array jobs, as one can start up a wide range of model trainings from a single launch script. Thirdly we can initialize the model from a JSON file created by the `to_json` method. Lastly we can use the `parse_model_helper_from_finetuned_name` factory function. This one takes a pretrained model name, created by the `get_finetuned_model_name(TrainingHelperD, TrainingHelperM)` function and returns with a `TrainingHelperM` object.\n",
    "\n",
    "- The `__init__(**kwargs)` method takes the following arguments:\n",
    "  - huggingface_model_name: (str) Required.\n",
    "  - epochs: (int) Optional. Defaults to 1.0\n",
    "  - learning_rate (float) optional. Defaults to 0.001.\n",
    "  - seq_len (int) Optional. Defaults to 512.\n",
    "  - batch_size (int) Optional. If not given it's inferred for known models, otherwise an error is thrown.\n",
    "  - gradient_accumulation_steps (int) Optional. If not given it's inferred for known models, otherwise an error is thrown.\n",
    "  Since many of the parameters are default or auto inferred those values might not be correct in every case! It is better practice to pass all known information to the `__init__` !\n",
    "\n",
    "- `initialize_from_environment()`: This is a class method factory function. It looks for the following environmental variables:\n",
    "  - `MODEL_NAME`: The full Hugging Face name of the model.\n",
    "  - `LEARNING_RATE`: The learning rate.\n",
    "  - `LS`: The maximal sequence length for the given training. (The model itself might be able to handle more!)\n",
    "  - `NUM_TRAIN_EPOCHS`: Number of training epochs.\n",
    "- `from_json(path)`: This class method loads the class from a JSON file.\n",
    "- `parse_model_helper_from_finetuned_name(name)`: Not recommended, as  it's unsafe. It only works for known models, as batch size and gradient accumulation steps are not part of the finetuned name, so they have to be auto inferred.\n",
    "\n",
    "## `TrainingHelperD`\n",
    "This class serves to store metadata regarding the training and testing dataset used as well as the task name the model was trained for.\n",
    "- `dataset_path`: Path to the dataset used for training and evaluation. These are Hugging Face datasets so this is a directory containing many splits.\n",
    "- `eval_split_name`: Name of the evaluation split.\n",
    "- `seq_len`: Maximum length of sequences in the dataset.\n",
    "- `task`: Name of the training task.\n",
    "- `test_dataset_path`: Absolute path to the testing dataset used. Split included!\n",
    "- `test_split_name`: Name of the test split.\n",
    "- `train_dataset_path`: Absolute path to the training dataset used. Split included!\n",
    "- `train_split_name`: Name of the training split.\n",
    "- `separator`: Short string used to separate metadata in the finetuned name. By default, it is: `___`.\n",
    "- `val_dataset_path`: Absolute path to the training dataset used. Split included!\n",
    "\n",
    "\n",
    "### Methods:\n",
    "This helper is yet to have any convenience methods. It's only two methods are for initialization. See below.\n",
    "\n",
    "### Initialization\n",
    "This helper class can be initialized through three methods: passing all variables, to its `__init__()`, calling the `initialize_from_environment` if the environmental variables are present, or loading from a JSON file by the `from_json` method.\n",
    "\n",
    "- `initialize_from_environment()`: This is a class method factory function. It looks for the following environmental variables:\n",
    "    - `DATASET_NAME`: Name of the testing dataset.\n",
    "    - `DATASET_PATH`: Absolute path to the dataset. Without the splits!\n",
    "    - `LS`: The maximal sequence length for the given training.\n",
    "    - `TASK`: Name of the training task.\n",
    "\n",
    "- `from_json(path)` Loads the model from a checkpoint file created by the `to_json` method.\n",
    "\n",
    "## `get_finetuned_model_name`\n",
    "This helper function generates the final model name. This is useful to ensure consistency, also making the model names themselves easily parsable. THe function takes two arguments: `get_finetuned_model_name(dataset_helper: TrainingHelperD, model_helper: TrainingHelperM)` and returns with the finetuned name as a string. The name follows this convention: `<DATASET_NAME>___<BASE_MODEL_NAME>___<TASK>___sl_<SEQ_LEN>___ep_<EPOCH>___lr_<LEARNING_RATE>`. The `___` separator string is used by default, but it can be overriden as this is a property of the `TrainingHelperD` class: `TrainingHelperD.separator`.\n",
    "\n",
    " # Now that we understand these classes let's try their functions so we gain a deeper insight.\n",
    " We will first initialize the classes n multiple ways then extract their properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_63vK3GQhZJt",
    "outputId": "51d4a325-f9fe-4254-e613-b8ccd99fb1cd"
   },
   "source": [
    "# This install method is guaranteed to work in google colab, so it is preferred for this example. For more details please check the Readme\n",
    "#!git clone --single-branch --branch TrainHelper https://github.com/nbrg-ppcu/prokbert.git\n",
    "#%pip install ./prokbert -q;"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x00eH-sS8zve",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "outputId": "cb93919f-5ae7-4ae3-9161-8ba0c92e4ab0"
   },
   "source": "from prokbert.traininghelper_utils import TrainingHelperM, TrainingHelperD, get_finetuned_model_name",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "URDUY1Gy9_YA"
   },
   "source": [
    "# Let's start by initializing the helpers through their __init__ methods\n",
    "\n",
    "model_helper = TrainingHelperM(\n",
    "    huggingface_model_name='neuralbioinfo/prokbert-mini-long')\n",
    "# Notice that no batch size or gradient accumulation steps is given. These are auto inferred since prokbert-mini-long is a known model\n",
    "# These batch sizes are calculated assuming 40GB NVIDIA A100-s\n",
    "del model_helper\n",
    "\n",
    "# To fully control parameters we can pass in everything through the arguments to init\n",
    "model_helper = TrainingHelperM(\n",
    "    huggingface_model_name='neuralbioinfo/prokbert-mini-long',\n",
    "    epochs=1,\n",
    "    learning_rate=0.001,\n",
    "    seq_len=512,\n",
    "    batch_size=64,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ycth0R1OAMY9"
   },
   "source": [
    "# To initialize the helpers from environmental variables we need to set them up first\n",
    "import os\n",
    "os.environ['MODEL_NAME'] = 'neuralbioinfo/prokbert-mini-long'\n",
    "os.environ['LEARNING_RATE'] = '0.001'\n",
    "os.environ['LS'] = '256'\n",
    "os.environ['NUM_TRAIN_EPOCHS'] = '1'\n",
    "os.environ['TASK'] = 'phage-lifestyle'\n",
    "os.environ['DATASET_PATH'] = '/tmp/testpath'\n",
    "os.environ['DATASET_NAME'] = 'testdataset'\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# This will work because prokbert-mini-long is a known model and BS and GAC are auto inferred\n",
    "del model_helper\n",
    "model_helper = TrainingHelperM.initialize_from_environment()"
   ],
   "metadata": {
    "id": "Ca8MNwdc8Kum"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "del model_helper\n",
    "os.environ['MODEL_NAME'] = 'some_developer/some_model'\n",
    "# This fails because it is not a known model\n",
    "model_helper = TrainingHelperM.initialize_from_environment()"
   ],
   "metadata": {
    "id": "c7dCVyXD8TSk"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# However if we add BS and GAC to the environmental variables the initialization will work\n",
    "os.environ['BATCH_SIZE'] = '64'\n",
    "os.environ['GRADIENT_ACCUMULATION_STEPS'] = '4'\n",
    "model_helper = TrainingHelperM.initialize_from_environment()"
   ],
   "metadata": {
    "id": "LCpLxBUa8T80"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lets save the helper so we can try loading it back\n",
    "model_helper.to_json('model_helper.json')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the helper from JSON\n",
    "helper_two = TrainingHelperM.from_json('model_helper.json')\n",
    "print(\"The helper loaded back from JSON is equal to the previous\", helper_two == model_helper)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's take a look at the dataset helper now\n",
    "\n",
    "dataset_helper = TrainingHelperD(\n",
    "    dataset_path='/path_to_some/dataset',\n",
    "    dataset_name='testdataset',\n",
    "    seq_len=512,\n",
    "    check_dataset_exist=False # This is needed because testdataset does not exist now\n",
    ") # Note that here we use the default task and separator here: phage and '___'\n",
    "\n",
    "# We can specify those explicitly. Doing so will change the finetuned name generated\n",
    "dataset_helper_two = TrainingHelperD(\n",
    "    dataset_path='/path_to_some/dataset',\n",
    "    dataset_name='testdataset',\n",
    "    seq_len=512,\n",
    "    check_dataset_exist=False,\n",
    "    separator='***',\n",
    "    task='phage-lifestyle',\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The dataset helper can be initialized from environment and JSON too\n",
    "del dataset_helper\n",
    "del dataset_helper_two\n",
    "dataset_helper = TrainingHelperD.initialize_from_environment(check_dataset_exist=False)\n",
    "dataset_helper.to_json('dataset_helper.json')\n",
    "dataset_helper_two = TrainingHelperD.from_json('dataset_helper.json')\n",
    "\n",
    "print(\"The helper loaded back from JSON is equal to the previous\", dataset_helper_two == dataset_helper)\n",
    "del dataset_helper_two"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Now that we know how to initialize and save the classes let's look at their usage\n",
    "Both classes are decorated with `@dataclass` so they have equivalence and ordering operators (==, <, >, <=, >=). Also since they are dataclasses they can be printed out directly."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's try printing\n",
    "print(model_helper)\n",
    "print(dataset_helper)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Something look off with those prints?\n",
    "As you might notice when printed like this the properties have a `_` prefix, and they are different, than described before. This is because these are the hidden internal values that are not supposed to be accessed or changed directly. The public properties of the models are all accessible through getter and setter methods, using the `@property` decorator.\n",
    "\n",
    "### So what can we access? These public properties\n",
    "Again the public properties of the helpers are the following:\n",
    "- `TrainingHelperM`:\n",
    "    - `HF_model_name`: This is the full Hugging Face identifier of the base model we are using.\n",
    "    - `huggingface_prefix`: The prefix string. Name of the developer team.\n",
    "    - `base_model_name`: This is only the name of the model without prefixes.\n",
    "    - `epochs`: Number of training epochs.\n",
    "    - `learning_rate`: Learning rate used to train the model.\n",
    "    - `seq_len`: Max length of the training sequences.\n",
    "    - `batch_size`: Training batch size. This is auto inferred for known model (see below at `__init__()`).\n",
    "    - `gradient_accumulation_steps`: Gradient accumulation steps. Auto-inferred too.\n",
    "- `TrainingHelperD`:\n",
    "    - `dataset_path`: Path to the dataset used for training and evaluation. These are Hugging Face datasets so this is a directory containing many splits.\n",
    "    - `eval_split_name`: Name of the evaluation split.\n",
    "    - `seq_len`: Maximum length of sequences in the dataset.\n",
    "    - `task`: Name of the training task.\n",
    "    - `test_dataset_path`: Absolute path to the testing dataset used. Split included!\n",
    "    - `test_split_name`: Name of the test split.\n",
    "    - `train_dataset_path`: Absolute path to the training dataset used. Split included!\n",
    "    - `train_split_name`: Name of the training split.\n",
    "    - `separator`: Short string used to separate metadata in the finetuned name. By default, it is: `___`.\n",
    "    - `val_dataset_path`: Absolute path to the training dataset used. Split included!\n",
    "\n",
    "A few things to note here. The names of the dataset splits are generated from the sequence length used. As a result these are non-modifiable. Trying to do so will raise an error. Consequently, the dataset paths are non-modifiable too!\n",
    "\n",
    "## Off to the more intriguing functionalities\n",
    "Here we will showcase the helper functions of these classes. Namely: `TrainingHelperM.get_default_model()`, `TrainingHelperM.get_tokenizer()`, `TrainingHelperM.get_tokenizer_fnction()` and `get_finetuned_model_name`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# These are more heavyweight operations it is recommended not to try these without a GPU\n",
    "\n",
    "model_helper = TrainingHelperM('neuralbioinfo/prokbert-mini-long') # Full default helper for prokbert\n",
    "\n",
    "model = model_helper.get_default_model() # This will return a Hugging Face AutoModelForClassification\n",
    "tokenizer = model_helper.get_tokenizer() # This returns the corresponding tokenizer\n",
    "tokenize_fn = model_helper.get_tokenizer_function() # Tokenizer function to use with dataset.map() for example"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's say we successfully trained out model and would like to save it\n",
    "finetuned_name = get_finetuned_model_name(dataset_helper=dataset_helper, model_helper=model_helper)\n",
    "\n",
    "model.save_pretrained(finetuned_name)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
