{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG59DW3yUyVJ"
   },
   "source": [
    "# Training Helper Utilities\n",
    "To ease the process of training and evaluation multiple models, we have implemented two helper classes `TrainingHelperM` and `TrainingHelperD`. These are metadata storage classes with many additional functionalities so the training scripts can be as uniform as possible.\n",
    "\n",
    "## `TrainingHelperM`\n",
    "This class stores metadata regarding the model we are training. Its attributes are the following:\n",
    "- `HF_model_name`: This is the Hugging Face identifier of the base model we are using.\n",
    "- `base_model_name`: This is only the name of the model without prefixes.\n",
    "- `epochs`: Number of training epochs.\n",
    "- `learning_rate`: Learning rate used to train the model.\n",
    "- `seq_len`: Max length of the training sequences.\n",
    "- `batch_size`: Training batch size. This is auto inferred for known model (see below at `__init__()`).\n",
    "- `gradient_accumulation_steps`: Gradient accumulation steps. Auto-inferred too.\n",
    "\n",
    "### Methods:\n",
    "`TrainingHelperM` has a few methods to further help the programmer:\n",
    "- `from_json(path)`: Loads the model from a JSON file (more below).\n",
    "- `get_default_model()`: This method returns a Hugging Face AutoModelForSequenceClassification instantiated from the name stored in the helper.\n",
    "- `get_tokenizer()`: Returns the matching tokenizer to the model.\n",
    "- `get_tokenizer_function()`: Returns a tokenizer function which can be used by `dataset.map()` for pre-tokenization and other tasks.\n",
    "- `initialize_from_environment()`: Instantiates the model helper from environmental variables (more below).\n",
    "- `initialize_from_finetuned_name(name)`: Instantiates the model helper from a finetuned name (more below).\n",
    "- `to_json(path)`: The helper can save its metadata to a JSON file.\n",
    "\n",
    "### Initialization of the class\n",
    "There are four ways to initialize a `TrainingHelperM` class. First off we can do so by calling it's `__init__()` method. Secondly we can use the `initialize_from_environment` class method, to pull the metadata from environment variables. This is extremely helpful for SLURM array jobs, as one can start up a wide range of model trainings from a single launch script. Thirdly we can initialize the model from a JSON file created by the `to_json` method. Lastly we can use the `parse_model_helper_from_finetuned_name` factory function. This one takes a pretrained model name, created by the `get_finetuned_model_name(TrainingHelperD, TrainingHelperM)` function and returns with a `TrainingHelperM` object.\n",
    "\n",
    "- The `__init__(**kwargs)` method takes the following arguments:\n",
    "  - huggingface_model_name (str)\n",
    "  - epochs (int)\n",
    "  - learning_rate (float)\n",
    "  - seq_len (int)\n",
    "  - batch_size (int) Optional. If not given it's inferred for known models, otherwise an error is thrown.\n",
    "  - gradient_accumulation_steps (int) Optional. If not given it's inferred for known models, otherwise an error is thrown.\n",
    "- `initialize_from_environment()`: This is a class method factory function. It looks for the following environmental variables:\n",
    "  - `MODEL_NAME`: The full Hugging Face name of the model.\n",
    "  - `LEARNING_RATE`: The learning rate.\n",
    "  - `LS`: The maximal sequence length for the given training. (The model itself might be able to handle more!)\n",
    "  - `NUM_TRAIN_EPOCHS`: Number of training epochs.\n",
    "- `from_json(path)`: This class method loads the class from a JSON file.\n",
    "- `parse_model_helper_from_finetuned_name(name)`: Not recommended, as  it's unsafe. It only works for known models, as batch size and gradient accumulation steps are not part of the finetuned name, so the y have to be auto inferred.\n",
    "\n",
    "## `TrainingHelperD`\n",
    "This class serves to store metadata regarding the training and testing dataset used as well as the task name the model was trained for.\n",
    "- `dataset_path`: Path to the dataset used for training and evaluation. These are Hugging Face datasets so this is a directory containing many splits.\n",
    "- `eval_split_name`: Name of the evaluation split.\n",
    "- `seq_len`: Maximum length of sequences in the dataset.\n",
    "- `task`: Name of the training task.\n",
    "- `test_dataset_path`: Absolute path to the testing dataset used. Split included!\n",
    "- `test_split_name`: Name of the test split.\n",
    "- `train_dataset_path`: Absolute path to the training dataset used. Split included!\n",
    "- `train_split_name`: Name of the training split.\n",
    "- `separator`: Short string used to separate metadata in the finetuned name. By default, it is: `___`.\n",
    "- `val_dataset_path`: Absolute path to the training dataset used. Split included!\n",
    "\n",
    "\n",
    "### Methods:\n",
    "This helper is yet to have any convenience methods. It's only two methods are for initialization. See below.\n",
    "\n",
    "### Initialization\n",
    "This helper class can be initialized through three methods: passing all variables, to its `__init__()`, calling the `initialize_from_environment` if the environmental variables are present, or loading from a JSON file by the `from_json` method.\n",
    "\n",
    "- `initialize_from_environment()`: This is a class method factory function. It looks for the following environmental variables:\n",
    "    - `DATASET_NAME`: Name of the testing dataset.\n",
    "    - `DATASET_PATH`: Absolute path to the dataset. Without the splits!\n",
    "    - `LS`: The maximal sequence length for the given training.\n",
    "    - `TASK`: Name of the training task.\n",
    "\n",
    "- `from_json(path)` Loads the model from a checkpoint file created by the `to_json` method.\n",
    "\n",
    "\n",
    " # Now that we understand these classes let's try their functions so we gain a deeper insight.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_63vK3GQhZJt",
    "outputId": "51d4a325-f9fe-4254-e613-b8ccd99fb1cd"
   },
   "source": [
    "# This install method is guaranteed to work in google colab, so it is preferred for this example. For more details please check the Readme\n",
    "#!git clone --single-branch --branch TrainHelper https://github.com/nbrg-ppcu/prokbert.git\n",
    "#%pip install ./prokbert -q;"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x00eH-sS8zve",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "outputId": "cb93919f-5ae7-4ae3-9161-8ba0c92e4ab0"
   },
   "source": [
    "from prokbert.traininghelper_utils import TrainingHelperM, TrainingHelperD"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "URDUY1Gy9_YA"
   },
   "source": [
    "# Let's start by initializing the helpers through their __init__ methods\n",
    "\n",
    "model_helper = TrainingHelperM(\n",
    "    huggingface_model_name='neuralbioinfo/prokbert-mini-long',\n",
    "    epochs=1,\n",
    "    learning_rate=0.001,\n",
    "    seq_len=512\n",
    ") # Notice that no batch size or gradient accumulation steps is given. These are auto inferred since prokbert-mini-long is a known model\n",
    "# These batch sizes are calculated assuming one is useing an NVIDIA A100\n",
    "# If we want to however we can pass in batch_size and gradient accumulation steps explicitly\n",
    "\n",
    "model_helper = TrainingHelperM(\n",
    "    huggingface_model_name='neuralbioinfo/prokbert-mini-long',\n",
    "    epochs=1,\n",
    "    learning_rate=0.001,\n",
    "    seq_len=512,\n",
    "    batch_size=64,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ycth0R1OAMY9"
   },
   "source": [
    "# To initialize the TrainingHelperM from environmental variables we need to set them up first\n",
    "import os\n",
    "os.environ['MODEL_NAME'] = 'neuralbioinfo/prokbert-mini-long'\n",
    "os.environ['LEARNING_RATE'] = '0.001'\n",
    "os.environ['LS'] = '256'\n",
    "os.environ['NUM_TRAIN_EPOCHS'] = '1'\n",
    "os.environ['TASK'] = 'phage-lifestyle'\n",
    "os.environ['DATASET_PATH'] = '/tmp/testpath'\n",
    "os.environ['DATASET_NAME'] = 'testdataset'\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# This will work because prokbert is a known model and BS and GAC are auto inferred\n",
    "model_helper = TrainingHelperM.initialize_from_environment()"
   ],
   "metadata": {
    "id": "Ca8MNwdc8Kum"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ['MODEL_NAME'] = 'some_developer/some_model'\n",
    "# This fails because it is not a known model\n",
    "conf_two = TrainingHelperM.initialize_from_environment()"
   ],
   "metadata": {
    "id": "c7dCVyXD8TSk"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# However if we add BS and GAC to the environmental variables the initialization will work\n",
    "os.environ['BATCH_SIZE'] = '64'\n",
    "os.environ['GRADIENT_ACCUMULATION_STEPS'] = '4'\n",
    "helper_two = TrainingHelperM.initialize_from_environment()"
   ],
   "metadata": {
    "id": "LCpLxBUa8T80"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lets save the helper so we can try loading it back\n",
    "model_helper.to_json('model_helper.json')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the helper from JSON\n",
    "helper_two = TrainingHelperM.from_json('model_helper.json')\n",
    "print(\"The helper loaded back from JSON is equal to the previous\", helper_two == model_helper)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's take a look at the dataset helper now\n",
    "\n",
    "dataset_helper = TrainingHelperD(\n",
    "    dataset_path='/path_to_some/dataset',\n",
    "    dataset_name='testdataset',\n",
    "    seq_len=512,\n",
    "    check_dataset_exist=False # This is needed because testdataset does not exist now\n",
    ") # Note that here we use the default task and separator here: phage and '___'\n",
    "\n",
    "dataset_helper_two = TrainingHelperD(\n",
    "    dataset_path='/path_to_some/dataset',\n",
    "    dataset_name='testdataset',\n",
    "    seq_len=512,\n",
    "    check_dataset_exist=False,\n",
    "    separator='***',\n",
    "    task='phage-lifestyle',\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The dataset helper can be initialized from environment and JSON too\n",
    "dataset_helper = TrainingHelperD.initialize_from_environment(check_dataset_exist=False)\n",
    "dataset_helper.to_json('dataset_helper.json')\n",
    "dataset_helper_two = TrainingHelperD.from_json('dataset_helper.json')\n",
    "\n",
    "print(\"The helper loaded back from JSON is equal to the previous\", dataset_helper_two == dataset_helper)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
