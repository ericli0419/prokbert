{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88222642-c405-4efd-beda-471c100cfe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 19:49:16,448 - INFO - Dataset size: 482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "PRETRAIN_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/pretraining.yaml\n",
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/sequence_processing.yaml\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "/home/ligeti/github/prokbert/src/prokbert\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "import pathlib\n",
    "from os.path import join\n",
    "import os\n",
    "import sys\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "current_path = str(pathlib.Path(os.getcwd()).parent)\n",
    "sys.path.append(current_path)\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('../)\n",
    "\n",
    "from config_utils import *\n",
    "from sequtils import *\n",
    "from training_utils import *\n",
    "from prokbert_tokenizer import ProkBERTTokenizer\n",
    "from prok_datasets import *\n",
    "from ProkBERTDataCollator import *\n",
    "    \n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "#pd.set_option('display.width', 4000)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "train_model_output_dir = ''\n",
    "train_model_name = 'LCA_repr_k6s1_l2048hs1536_h16_l16_mlm02_rp8_rndp1_rkv'\n",
    "\n",
    "model_params = {'model_outputpath': '/scratch/fastscratch/NBL/trained_models/test/mini0',\n",
    "                'model_name' : 'mini0'}\n",
    "dataset_params = {'dataset_path' : '../data/preprocessed/pretraining.h5'}\n",
    "pretraining_params = {}\n",
    "\n",
    "\n",
    "prokbert_config = ProkBERTConfig()\n",
    "_ = prokbert_config.get_and_set_model_parameters(model_params)\n",
    "_ = prokbert_config.get_and_set_dataset_parameters(dataset_params)\n",
    "_ = prokbert_config.get_and_set_pretraining_parameters(pretraining_params)\n",
    "\n",
    "\n",
    "# Kell egy dataset\n",
    "hdf_file = '../data/preprocessed/pretraining.h5'\n",
    "\n",
    "ds = IterableProkBERTPretrainingDataset(hdf_file, input_batch_size=10000, max_iteration_over_ds=5)\n",
    "\n",
    "X = ds[0:3][:,0:10]\n",
    "#torch.stack(ds[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f46d4fef-2cc0-4684-94a0-62a6ca8bb906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/sequence_processing.yaml\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "Loading the datacollator class!\n",
      "Collator Parameters:\n",
      "  Number of tokens masked to left:  3\n",
      "  Number of tokens masked to right: 2\n",
      "  Probability of restoring a masked token: 0.8\n",
      "  Probability of changing to a random token: 0.01\n",
      "  MLM Probability: 0.05\n",
      "  Default token type: torch.int16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_training_tokenizer(prokbert_config)\n",
    "prokbert_dc = get_data_collator_for_overlapping_sequences(tokenizer, prokbert_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c74a59b8-f668-41ba-b5c7-2e27f3c878c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'mini0',\n",
       " 'model_outputpath': '/scratch/fastscratch/NBL/trained_models/test/mini0',\n",
       " 'vocab_size': 4101,\n",
       " 'hidden_size': 384,\n",
       " 'num_hidden_layers': 6,\n",
       " 'num_attention_heads': 6,\n",
       " 'max_position_embeddings': 1024,\n",
       " 'intermediate_size': 2048,\n",
       " 'position_embedding_type': 'relative_key_query',\n",
       " 'ResumeTraining': True,\n",
       " 'resume_or_initiation_model_path': ''}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path: /scratch/fastscratch/NBL/trained_models/test/mini0\n",
      "/scratch/fastscratch/NBL/trained_models/test/mini0\n",
      "The 0 is the largest checkpoint!\n"
     ]
    }
   ],
   "source": [
    "prokbert_config.model_params\n",
    "\n",
    "[m_exists, cp_dir, cp, cps] = check_model_existance_and_checkpoint(prokbert_config.model_params['model_outputpath'], \n",
    "                                     prokbert_config.model_params['model_name'])\n",
    "# Conclusion: The current model extsts and can be resumed. \n",
    "\n",
    "# Cheking initiation model, if training required\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1befd96-abdd-4469-a761-c4dcc0f5708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 19:49:30,559 - INFO - Dataset size: 482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether the file exists or not!\n",
      "Loading and creating a IterableProkBERTPretrainingDataset\n"
     ]
    }
   ],
   "source": [
    "# checking hdf dataset\n",
    "hdf_file_exists, ds_size = check_hdf_dataset_file(prokbert_config)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8527b77c-e10c-4d4e-84d3-7053c41ec43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_path': '',\n",
       " 'pretraining_dataset_data': [[]],\n",
       " 'dataset_class': 'IterableProkBERTPretrainingDataset',\n",
       " 'input_batch_size': 10000,\n",
       " 'dataset_iteration_batch_offset': 0,\n",
       " 'max_iteration_over_dataset': 10}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prokbert_config.dataset_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfd702-cbc5-450c-9867-a3da2e412b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d9432-afff-494d-aa41-5ef51e714a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5babd-c5d9-4293-bd94-640aae2db126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5a0bd8-5a1c-4803-a448-c66e41547814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datacollator class!\n",
      "Collator Parameters:\n",
      "  Number of tokens masked to left:  3\n",
      "  Number of tokens masked to right: 2\n",
      "  Probability of restoring a masked token: 0.8\n",
      "  Probability of changing to a random token: 0.01\n",
      "  MLM Probability: 0.05\n",
      "  Default token type: torch.int16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading the datacollator class!')\n",
    "prokbert_dc = ProkBERTDataCollator(tokenizer,\n",
    "                                  mask_to_left=prokbert_config.data_collator_params['mask_to_left'], \n",
    "                                  mask_to_right=prokbert_config.data_collator_params['mask_to_right'],\n",
    "                                  mlm_probability =   prokbert_config.data_collator_params['mlm_probability'],\n",
    "                                  replace_prob =prokbert_config.data_collator_params['replace_prob'],\n",
    "                                  random_prob = prokbert_config.data_collator_params['random_prob'])\n",
    "print(str(prokbert_dc))\n",
    "\n",
    "inputs, labels = prokbert_dc.torch_mask_tokens(ds[0:3][:,0:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73ec47-804e-4155-ab3c-08849c773eee",
   "metadata": {},
   "source": [
    "### Pretraining folyamata\n",
    "\n",
    "Vannak előkészítő feladatok:\n",
    "  * tokenizáló loadolása\n",
    "  * collator osztály loadolása\n",
    "  * Meglévő modellek ellenőrzése, checkpoint-ok ellenőrzése\n",
    "  * dataset betöltése és inicializálása, ellenőrzése (megfelel-e a követelményeknek vagy sem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2a7794-694e-46d1-a6af-ba61394f2d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/sequence_processing.yaml\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "Loading the datacollator class!\n",
      "Collator Parameters:\n",
      "  Number of tokens masked to left:  3\n",
      "  Number of tokens masked to right: 2\n",
      "  Probability of restoring a masked token: 0.8\n",
      "  Probability of changing to a random token: 0.01\n",
      "  MLM Probability: 0.05\n",
      "  Default token type: torch.int16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_training_tokenizer(prokbert_config)\n",
    "prokbert_dc = get_data_collator_for_overlapping_sequences(tokenizer, prokbert_config)\n",
    "\n",
    "#inputs, labels = prokbert_dc.torch_mask_tokens(sequences)\n",
    "# Kell egy paraméter szet a computation-nek, tokenizer-nek, stb. \n",
    "\n",
    "# Paraméterszett a datacollator-nak\n",
    "# Paraméterszett a modell-nek\n",
    "# Paraméterszett a training-nek. \n",
    "# IsResumeTraining és ilyenek\n",
    "\n",
    "\n",
    "#prokbert_config.computation_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06ee3c5-a01e-4049-ad4c-2a6ff1cb47a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prokbert_config.default_torchtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a78975-d81b-4e24-b072-60c9f9ac23c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129c4a2-1dad-434e-97d7-06482b3e1ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
