{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f332ed85-8a44-4acb-94f5-79aadf4c611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 13:39:16,580 - INFO - Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-08-17 13:39:16,581 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#import sys\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#sys.path.append('../)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msequtils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprokbert_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProkBERTTokenizer\n\u001b[1;32m     22\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10000\u001b[39m)\n",
      "File \u001b[0;32m~/github/prokbert/src/prokbert/sequtils.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Type, Tuple\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m product\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Ezt a felhasználónak kellene biztosatania \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "import pathlib\n",
    "from os.path import join\n",
    "import os\n",
    "import sys\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "current_path = str(pathlib.Path(os.getcwd()).parent)\n",
    "sys.path.append(current_path)\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('../)\n",
    "\n",
    "from config_utils import *\n",
    "from sequtils import *\n",
    "from prokbert_tokenizer import ProkBERTTokenizer\n",
    "    \n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "#pd.set_option('display.width', 4000)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "defconfig = SeqConfig()\n",
    "         \n",
    "segmentation_params = {'type': 'random'}\n",
    "tokenization_params = {'shift':2} \n",
    "                   \n",
    "comp_params = defconfig.get_set_computational_paramters()\n",
    "tokenizer = ProkBERTTokenizer(tokenization_params=tokenization_params, \n",
    "                              segmentation_params=segmentation_params,\n",
    "                              operation_space='sequence')\n",
    "segment = 'AATCAAGGAATTATTATCGTT'\n",
    "       \n",
    "segmentation_params = defconfig.get_set_segmentation_parameters()\n",
    "tokenization_params = defconfig.get_and_set_tokenization_params({'shift':2})             \n",
    "comp_params = defconfig.get_set_computational_paramters()\n",
    "\n",
    "num_cores = comp_params['cpu_cores_for_tokenization']\n",
    "batch_size = comp_params['batch_size_tokenization']\n",
    "numpy_dtype=comp_params['np_tokentype'] # Note that if you use kmer>7 then other is recommended\n",
    "input_fasta_dir = join(current_path, 'data/sample_data/pretraining')\n",
    "\n",
    "input_fasta_files = [join(input_fasta_dir, file) for file in get_non_empty_files(input_fasta_dir)]\n",
    "contigs = load_contigs(input_fasta_files,IsAddHeader=True,AsDataFrame=True, adding_reverse_complement=False)\n",
    "contigs['sequence_id'] = list(range(len(contigs)))\n",
    "contigs = contigs[['sequence', 'sequence_id']]\n",
    "segment_db = segment_sequences(contigs, segmentation_params, AsDataFrame=True)\n",
    "\n",
    "tokenized = batch_tokenize_segments_with_ids(segment_db, tokenization_params, num_cores, batch_size, numpy_dtype)\n",
    "expected_max_token = max(len(arr) for arrays in tokenized.values() for arr in arrays)\n",
    "X, torchdb = get_rectangular_array_from_tokenized_dataset(tokenized,\n",
    "                                                          tokenization_params['shift'],\n",
    "                                                          expected_max_token, numpy_dtype = numpy_dtype)\n",
    "hdf_file = '../data/preprocessed/pretraining.h5'\n",
    "save_to_hdf(X, hdf_file, database = torchdb, compression=True)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "953d31b8-83e2-465b-ab55-c9ada9237871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 09:38:22,001 - INFO - Dataset size: 482\n",
      "2023-08-14 09:38:22,010 - INFO - Stopping the iteration.\n"
     ]
    }
   ],
   "source": [
    "# Check hdf file\n",
    "from prok_datasets import IterableProkBERTPretrainingDataset, ProkBERTPretrainingHDFDataset, ProkBERTPretrainingDataset\n",
    "\n",
    "ds = IterableProkBERTPretrainingDataset(hdf_file, input_batch_size=482, max_iteration_over_ds=2)\n",
    "\n",
    "for i, r in enumerate(ds):\n",
    "    #r[0:10]\n",
    "    pass\n",
    "\n",
    "ds = ProkBERTPretrainingHDFDataset(hdf_file)\n",
    "#len(ds)\n",
    "#ds[0:2]\n",
    "ds = ProkBERTPretrainingDataset(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40b35f-4a1d-4062-8836-90a0066ee514",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.floor(3.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8003658-666a-429e-bf2e-5525bb6002ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf_file, 'r') as dataset_file:\n",
    "    dataset_file['training_data']['X'].shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05149138-e5a1-41b7-9e98-7e2f681c5f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 09:28:11,629 - INFO - Dataset size: 482\n",
      "Exception ignored in: <function IterableProkBERTPretrainingDataset.__del__ at 0x7ff3886c5a60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ligeti/github/prokbert/src/prokbert/prok_datasets.py\", line 125, in __del__\n",
      "    if not self.dataset_file.closed:\n",
      "AttributeError: 'File' object has no attribute 'closed'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([   2, 3329, 4038, 3103,  427, 2667, 1639, 1587,  753, 3784, 3130,  867,\n",
       "         1514, 3671, 1331,  745, 3653, 1029,    8,   57,  838, 1045,  266,   86,\n",
       "         1314,  484, 3573, 3851,  106, 1622, 1314,  478, 3482, 2396, 1411, 2036,\n",
       "         3833, 3911, 1063,  556,  642, 2005, 3341,  136, 2107,  869, 1545,   72,\n",
       "         1077,  774,   22,  291,  492, 3704, 1853,  910, 2199, 2354,  730, 3414,\n",
       "         1301,  265,   79, 1193, 2642, 1248, 3521, 3015, 3118,  672, 2499, 3049,\n",
       "         3655, 1069,  646, 2078,  419, 2537, 3665, 1228, 3196, 1918, 1941, 2316,\n",
       "          123, 1902, 1697, 2514, 3291, 3443, 1772, 3714, 2013, 3468, 2178, 2012,\n",
       "         3454, 1941, 2322,  228, 3586, 4060, 3454, 1942, 2325,  275,  234, 3684,\n",
       "         1540, 4093, 3980, 2170, 1891, 1510, 3607,  304,  696, 2879,  947, 2804,\n",
       "         3833, 3919, 1192, 2619,  872, 1598,  927, 2472, 2619,  870, 1559,  297,\n",
       "          582, 1058,  478, 3487, 2472, 2619,  870, 1570,  473, 3405, 1164, 2173,\n",
       "         1925, 2058,   97, 1488, 3267, 3058, 3811, 3566, 3736, 2367,  939, 2669,\n",
       "         1681, 2245, 3092,  258, 4053, 3346,  222, 3491, 2541, 3725, 2194, 2270,\n",
       "         3492, 2562, 4063, 3505, 2769, 3281, 3279, 3243, 2661, 1542,   26,  342,\n",
       "         1308,  386, 2007, 3377,  715, 3177, 1612, 1154, 2008, 3390,  930, 2521,\n",
       "         3410, 1240, 3388,  894, 1955, 2548, 3844, 4098, 4064, 3509, 2833,  198,\n",
       "         3101,  392, 2107,  883, 1778, 3807, 3504, 2752, 3009, 3020, 3202, 2011,\n",
       "         3434, 1624, 1348, 1028, 4094, 3993, 2382, 1186, 2518, 3353,  328, 1086,\n",
       "          925, 2449, 2249, 3153, 1225, 3153, 1228, 3194, 1878, 1313,  464, 3265,\n",
       "         3026, 3296, 3523, 3048, 3652, 1027, 4082, 3801, 3398, 1045,  268,  131,\n",
       "         2034, 3799,    3,    0], dtype=torch.int32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyError",
     "evalue": "'Unable to synchronously open object (invalid identifier type to function)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m ds\u001b[38;5;241m.\u001b[39mdataset_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#ds = IterableProkBERTPretrainingDataset(hdf_file, input_batch_size=482, max_iteration_over_ds=2)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/github/prokbert/src/prokbert/prok_datasets.py:118\u001b[0m, in \u001b[0;36mIterableProkBERTPretrainingDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_file[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_data\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m][index], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint16)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Return slice\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mtensor(item, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m][index]]\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/fastscratch/NBL/ptbin/miniconda3/lib/python3.9/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:190\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Unable to synchronously open object (invalid identifier type to function)'"
     ]
    }
   ],
   "source": [
    "ds = IterableProkBERTPretrainingDataset(hdf_file, input_batch_size=482, max_iteration_over_ds=2)\n",
    "\n",
    "ds[0:1]\n",
    "ds.dataset_file.close()\n",
    "#ds = IterableProkBERTPretrainingDataset(hdf_file, input_batch_size=482, max_iteration_over_ds=2)\n",
    "\n",
    "ds[0:3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e31c0590-c429-4efc-ad65-bc168ffafe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 09:32:07,657 - INFO - Dataset size: 482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = IterableProkBERTPretrainingDataset(hdf_file, input_batch_size=482, max_iteration_over_ds=2)\n",
    "ds.dataset_file.close()\n",
    "\n",
    "ds._ensure_file_open()\n",
    "\n",
    "ds.dataset_file.mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e06356b3-756b-4697-a760-ae4a35dde2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = h5py.File(hdf_file, 'r')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9db1914-55ed-4c23-8813-b22c81384e84",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'File' object has no attribute 'closed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosed\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'File' object has no attribute 'closed'"
     ]
    }
   ],
   "source": [
    "dataset_file.closed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015570ab-c415-4668-805e-5b5d57ed4402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
