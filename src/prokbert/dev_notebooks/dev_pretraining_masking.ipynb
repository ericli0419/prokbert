{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f332ed85-8a44-4acb-94f5-79aadf4c611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 08:39:36,393 - INFO - Loading sequence data into memory!\n",
      "2023-08-14 08:39:36,398 - INFO - Checking input DataFrame!\n",
      "2023-08-14 08:39:36,399 - INFO - Checking input sequence_id is valid primary key in the DataFrame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/sequence_processing.yaml\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/sequence_processing.yaml\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "/home/ligeti/github/prokbert/src/prokbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 08:39:36,673 - INFO - Tokenization of a list of segments\n",
      "2023-08-14 08:39:36,896 - INFO - Doing randomization!\n",
      "2023-08-14 08:39:36,899 - INFO - Tuncating all zeros column\n",
      "2023-08-14 08:39:36,900 - INFO - Existing HDF5 file ../data/preprocessed/pretraining.h5 removed successfully.\n",
      "2023-08-14 08:39:36,906 - INFO - Numpy array saved to ../data/preprocessed/pretraining.h5 successfully.\n",
      "2023-08-14 08:39:36,906 - INFO - Adding database into the HDF5 file!\n",
      "2023-08-14 08:39:36,907 - INFO - Number of chunks: 1\n",
      "2023-08-14 08:39:36,908 - INFO - Writing database chunk 0 into ../data/preprocessed/pretraining.h5\n",
      "2023-08-14 08:39:36,958 - INFO - Database addition finished!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "import pathlib\n",
    "from os.path import join\n",
    "import os\n",
    "import sys\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "current_path = str(pathlib.Path(os.getcwd()).parent)\n",
    "sys.path.append(current_path)\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('../)\n",
    "\n",
    "from config_utils import *\n",
    "from sequtils import *\n",
    "from prokbert_tokenizer import ProkBERTTokenizer\n",
    "    \n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "#pd.set_option('display.width', 4000)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "defconfig = SeqConfig()\n",
    "         \n",
    "segmentation_params = {'type': 'random'}\n",
    "tokenization_params = {'shift':2} \n",
    "                   \n",
    "comp_params = defconfig.get_set_computational_paramters()\n",
    "tokenizer = ProkBERTTokenizer(tokenization_params=tokenization_params, \n",
    "                              segmentation_params=segmentation_params,\n",
    "                              operation_space='sequence')\n",
    "segment = 'AATCAAGGAATTATTATCGTT'\n",
    "       \n",
    "segmentation_params = defconfig.get_set_segmentation_parameters()\n",
    "tokenization_params = defconfig.get_and_set_tokenization_params({'shift':2})             \n",
    "comp_params = defconfig.get_set_computational_paramters()\n",
    "\n",
    "num_cores = comp_params['cpu_cores_for_tokenization']\n",
    "batch_size = comp_params['batch_size_tokenization']\n",
    "numpy_dtype=comp_params['np_tokentype'] # Note that if you use kmer>7 then other is recommended\n",
    "input_fasta_dir = join(current_path, 'data/sample_data/pretraining')\n",
    "\n",
    "input_fasta_files = [join(input_fasta_dir, file) for file in get_non_empty_files(input_fasta_dir)]\n",
    "contigs = load_contigs(input_fasta_files,IsAddHeader=True,AsDataFrame=True, adding_reverse_complement=False)\n",
    "contigs['sequence_id'] = list(range(len(contigs)))\n",
    "contigs = contigs[['sequence', 'sequence_id']]\n",
    "segment_db = segment_sequences(contigs, segmentation_params, AsDataFrame=True)\n",
    "\n",
    "tokenized = batch_tokenize_segments_with_ids(segment_db, tokenization_params, num_cores, batch_size, numpy_dtype)\n",
    "expected_max_token = max(len(arr) for arrays in tokenized.values() for arr in arrays)\n",
    "X, torchdb = get_rectangular_array_from_tokenized_dataset(tokenized,\n",
    "                                                          tokenization_params['shift'],\n",
    "                                                          expected_max_token, numpy_dtype = numpy_dtype)\n",
    "hdf_file = '../data/preprocessed/pretraining.h5'\n",
    "save_to_hdf(X, hdf_file, database = torchdb, compression=True)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "953d31b8-83e2-465b-ab55-c9ada9237871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 08:53:16,478 - INFO - Dataset size: 482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer: 0\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Get next interval info\n",
      "Fetched intervals: new_fetch_start: 0 new_fetch_end: 482\n",
      "Stoppfing the iteration!\n"
     ]
    }
   ],
   "source": [
    "# Check hdf file\n",
    "from prok_datasets import IterableProkBERTPretrainingDataset\n",
    "\n",
    "ds = IterableProkBERTPretrainingDataset(hdf_file, input_batch_size=482)\n",
    "\n",
    "for r in ds:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d40b35f-4a1d-4062-8836-90a0066ee514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(3.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8003658-666a-429e-bf2e-5525bb6002ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with h5py.File(hdf_file, 'r') as dataset_file:\n",
    "    dataset_file['training_data']['X'].shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05149138-e5a1-41b7-9e98-7e2f681c5f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
