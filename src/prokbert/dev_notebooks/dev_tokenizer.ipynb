{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab9e784-8ddf-4b59-8379-3f879c0ae9b1",
   "metadata": {},
   "source": [
    "# Tokenizer dev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f2e15d-3056-412c-bbcc-0661c3af360b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/sequence_processing.yaml\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/sequence_processing.yaml\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "_____\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "import pathlib\n",
    "from os.path import join\n",
    "import os\n",
    "import sys\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "current_path = str(pathlib.Path(os.getcwd()).parent)\n",
    "sys.path.append(current_path)\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('../)\n",
    "\n",
    "from config_utils import *\n",
    "from sequtils import *\n",
    "from prokbert_tokenizer import ProkBERTTokenizer\n",
    "    \n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "#pd.set_option('display.width', 4000)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "defconfig = SeqConfig()\n",
    "         \n",
    "segmentation_params = {'type': 'random'}\n",
    "tokenization_params = {'shift':1} \n",
    "                   \n",
    "comp_params = defconfig.get_and_set_computational_parameters()\n",
    "tokenizer = ProkBERTTokenizer(tokenization_params=tokenization_params, \n",
    "                              segmentation_params=segmentation_params,\n",
    "                              operation_space='sequence')\n",
    "\n",
    "\n",
    "#print(tokenizer.cls_token)\n",
    "#print(tokenizer.cls_token_id)\n",
    "print('_____')\n",
    "print(tokenizer.vocab[tokenizer.cls_token] )\n",
    "tokenizer.cls_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226e25e-cbd3-4ec8-ad08-00204e77eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "AATCAAGGAATTATTATCGTT\n",
    " ATCAAGGAATTATTATCGTT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479b39de-30e6-413f-9020-4721f7e92199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 14:24:30,024 - INFO - Nr. line to cover the seq:  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment sequence AATCAAGGAATTATTATCGTT\n",
      "Tokens:   \n",
      "[2, 213, 839, 3343, 1069, 165, 648, 2580, 2113, 248, 980, 3905, 3320, 978, 3899, 3296, 884, 3]\n",
      "___________________\n",
      "Kmers:   \n",
      "['AATCAA', 'ATCAAG', 'TCAAGG', 'CAAGGA', 'AAGGAA', 'AGGAAT', 'GGAATT', 'GAATTA', 'AATTAT', 'ATTATT', 'TTATTA', 'TATTAT', 'ATTATC', 'TTATCG', 'TATCGT', 'ATCGTT']\n",
      "___________________\n",
      "Restore the original string:\n",
      "['AATCAA']\n",
      "    AATCAAGGAATTATTATCGTT\n",
      "    AATCAAGGAATTATTATCGTT\n",
      "0.  AATCAA  AATTAT\n",
      "1.   ATCAAG  ATTATT\n",
      "2.    TCAAGG  TTATTA\n",
      "3.     CAAGGA  TATTAT\n",
      "4.      AAGGAA  ATTATC\n",
      "5.       AGGAAT  TTATCG\n",
      "6.        GGAATT  TATCGT\n",
      "7.         GAATTA  ATCGTT\n",
      "Checking, windows = 0\n",
      "['AATCAA', 'ATCAAG', 'TCAAGG', 'CAAGGA', 'AAGGAA', 'AGGAAT', 'GGAATT', 'GAATTA', 'AATTAT', 'ATTATT', 'TTATTA', 'TATTAT', 'ATTATC', 'TTATCG', 'TATCGT', 'ATCGTT']\n",
      "Checking, all == True\n",
      "([[2, 213, 839, 3343, 1069, 165, 648, 2580, 2113, 248, 980, 3905, 3320, 978, 3899, 3296, 884, 3]], [['AATCAA', 'ATCAAG', 'TCAAGG', 'CAAGGA', 'AAGGAA', 'AGGAAT', 'GGAATT', 'GAATTA', 'AATTAT', 'ATTATT', 'TTATTA', 'TATTAT', 'ATTATC', 'TTATCG', 'TATCGT', 'ATCGTT']])\n"
     ]
    }
   ],
   "source": [
    "      \n",
    "segment = 'AATCAAGGAATTATTATCGTT'\n",
    "print(f'Segment sequence {segment}')\n",
    "tokens, kmers = tokenizer.tokenize(segment, all=True)\n",
    "print('Tokens:   ')\n",
    "for tokenset in tokens:\n",
    "       print(str(tokenset))\n",
    "print('___________________')\n",
    "         \n",
    "print('Kmers:   ')\n",
    "for tokenset in kmers:\n",
    "       print(str(tokenset))\n",
    "print('___________________')\n",
    "         \n",
    "         \n",
    "print('Restore the original string:')\n",
    "seq_toks = tokenizer.convert_ids_to_tokens(tokens[0])\n",
    "print('    ' + ''.join(seq_toks))\n",
    "\n",
    "s = pretty_print_overlapping_sequence(segment, kmers[0], tokenizer.tokenization_params)\n",
    "print(s)\n",
    "\n",
    "print('Checking, windows = 0')\n",
    "lca_kmers = tokenizer.tokenize(segment, lca_shift=0)\n",
    "print(str(lca_kmers))\n",
    "\n",
    "print('Checking, all == True')\n",
    "lca_kmers = tokenizer.tokenize(segment, all=True)\n",
    "print(str(lca_kmers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca924a-8307-4459-b0bf-eb74e778035c",
   "metadata": {},
   "source": [
    "## Checking for k-mer space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e12bb-316d-4c13-9049-a7736adcf561",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_k6_s1 = ProkBERTTokenizer(tokenization_params={'kmer': 6, 'shift': 1}, operation_space='sequence')\n",
    "tokenizer_k6_s2 = ProkBERTTokenizer(tokenization_params={'kmer': 6, 'shift': 2}, operation_space='sequence')\n",
    "segment = 'AATCAAGGAATTATTATCGTT' \n",
    "\n",
    "kmer_list = tokenizer_k6_s1.tokenize(segment, lca_shift=0)\n",
    "print(kmer_list)\n",
    "\n",
    "print(tokenizer_k6_s1.encode(segment))\n",
    "\n",
    "token_ids = tokenizer_k6_s2.encode(segment, all=True, add_special_tokens=False)\n",
    "token_ids = tokenizer_k6_s2.encode(segment)\n",
    "\n",
    "print(token_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915082f-85f0-4c5a-ae8e-9c03d1f5c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_k6_s2.vocab[tokenizer_k6_s2.cls_token]\n",
    "tokenizer_k6_s2.cls_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc3ff0-e854-4d0e-94d8-c5c497646ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f3024-60e9-43d7-a588-1385d000ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Listát csinál belőle. De előbb nekünk ebből k-mer listák kellenének, ahogy az meg van írva\n",
    "segment = 'AATCAAGGAATTATTATCGTT'\n",
    "\n",
    "tokens, kmers = tokenizer.tokenize(segment, all=True)\n",
    "#tokenizer.id2token\n",
    "#tokenizer = ProkBERTTokenizer(operation_space='sequence')\n",
    "#tokenizer.id2token.get(10, 'NN')\n",
    "#tokenizer.id2token.get(10, 'NN')[-2:]\n",
    "#print(tokens[0])\n",
    "#print(kmers[0])\n",
    "#print(kmers[0])\n",
    "seq_toks = tokenizer.convert_ids_to_tokens(tokens[0])\n",
    "seq_toks\n",
    "# Nem tudja lefedni, akkor az uccsó az hiányozhat, mivel nem teljesen egyértelmű, hogy pontosan mi is van. \n",
    "#print(''.join(seq_toks))\n",
    "#print(segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f223f-2268-4d5d-ad43-ec247768db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens[0]\n",
    "#kmers[0]\n",
    "#\n",
    "s = pretty_print_overlapping_sequence(segment, kmers[0], tokenizer.tokenization_params)\n",
    "#print(s)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e1b31cf-7a8f-4727-a378-ea802399e687",
   "metadata": {},
   "source": [
    "    AATTAAGGAATTATTATCGT\n",
    "    AATTAAGGAATTATTATCGT\n",
    "0.  AATTAA  AATTAT\n",
    "1.    TTAAGG  TTATTA\n",
    "2.      AAGGAA  ATTATC\n",
    "3.        GGAATT  TATCGT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383bbf0-dbac-4e8d-a10f-e2b0b13a212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = ['AASTTAAGGAATTATTATCGT', 'TCCGTAASTTAAGGAATTATTATCGT']\n",
    "tokenizer.batch_encode_plus(seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f11e46-1727-49e9-8803-9315476a0c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
