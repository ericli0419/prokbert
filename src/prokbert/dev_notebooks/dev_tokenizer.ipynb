{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab9e784-8ddf-4b59-8379-3f879c0ae9b1",
   "metadata": {},
   "source": [
    "# Tokenizer dev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f2e15d-3056-412c-bbcc-0661c3af360b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 12:34:32,167 - INFO - Nr. line to cover the seq:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: C:\\Users\\uif34392\\Documents\\JupyterLab\\GitHub\\prokbert\\src\\prokbert\\configs\\sequence_processing.yaml\n",
      "C:\\Users\\uif34392\\Documents\\JupyterLab\\GitHub\\prokbert\\src\\prokbert\n",
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: C:\\Users\\uif34392\\Documents\\JupyterLab\\GitHub\\prokbert\\src\\prokbert\\configs\\sequence_processing.yaml\n",
      "C:\\Users\\uif34392\\Documents\\JupyterLab\\GitHub\\prokbert\\src\\prokbert\n",
      "C:\\Users\\uif34392\\Documents\\JupyterLab\\GitHub\\prokbert\\src\\prokbert\n",
      "Segment sequence AATCAAGGAATTATTATCGTT\n",
      "Tokens:   \n",
      "[2, 213, 3343, 165, 2580, 248, 3905, 978, 3296, 3]\n",
      "[2, 839, 1069, 648, 2113, 980, 3320, 3899, 884, 3]\n",
      "___________________\n",
      "Kmers:   \n",
      "['AATCAA', 'TCAAGG', 'AAGGAA', 'GGAATT', 'AATTAT', 'TTATTA', 'ATTATC', 'TATCGT']\n",
      "['ATCAAG', 'CAAGGA', 'AGGAAT', 'GAATTA', 'ATTATT', 'TATTAT', 'TTATCG', 'ATCGTT']\n",
      "___________________\n",
      "Restore the original string:\n",
      "[]\n",
      "['AATCAA']\n",
      "    AATCAAGGAATTATTATCGT\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "    AATCAAGGAATTATTATCGTT\n",
      "0.  A  A\n",
      "1.    A  A\n",
      "2.      T\n",
      "3.        C\n",
      "\n",
      "\n",
      "0.   T  G\n",
      "1.     C  G\n",
      "2.       A\n",
      "3.         A\n",
      "\n",
      "\n",
      "0.    A  A\n",
      "1.      A  A\n",
      "2.        G\n",
      "3.          G\n",
      "\n",
      "\n",
      "0.     G  T\n",
      "1.       G  T\n",
      "2.         A\n",
      "3.           A\n",
      "\n",
      "\n",
      "0.      A  A\n",
      "1.        A  T\n",
      "2.          T\n",
      "3.            T\n",
      "\n",
      "\n",
      "0.       T  T\n",
      "1.         T  A\n",
      "2.           A\n",
      "3.             T\n",
      "\n",
      "\n",
      "0.        A  T\n",
      "1.          T  C\n",
      "2.            T\n",
      "3.              A\n",
      "\n",
      "\n",
      "0.         T  G\n",
      "1.           A  T\n",
      "2.             T\n",
      "3.               C\n",
      "\n",
      "\n",
      "Checking, windows = 0\n",
      "['AATCAA', 'TCAAGG', 'AAGGAA', 'GGAATT', 'AATTAT', 'TTATTA', 'ATTATC', 'TATCGT']\n",
      "Checking, all == True\n",
      "([[2, 213, 3343, 165, 2580, 248, 3905, 978, 3296, 3], [2, 839, 1069, 648, 2113, 980, 3320, 3899, 884, 3]], [['AATCAA', 'TCAAGG', 'AAGGAA', 'GGAATT', 'AATTAT', 'TTATTA', 'ATTATC', 'TATCGT'], ['ATCAAG', 'CAAGGA', 'AGGAAT', 'GAATTA', 'ATTATT', 'TATTAT', 'TTATCG', 'ATCGTT']])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "import pathlib\n",
    "from os.path import join\n",
    "import os\n",
    "import sys\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "current_path = str(pathlib.Path(os.getcwd()).parent)\n",
    "sys.path.append(current_path)\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('../)\n",
    "\n",
    "from config_utils import *\n",
    "from sequtils import *\n",
    "from prokbert_tokenizer import ProkBERTTokenizer\n",
    "    \n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "#pd.set_option('display.width', 4000)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "defconfig = SeqConfig()\n",
    "         \n",
    "segmentation_params = {'type': 'random'}\n",
    "tokenization_params = {'shift':2} \n",
    "                   \n",
    "comp_params = defconfig.get_set_computational_paramters()\n",
    "tokenizer = ProkBERTTokenizer(tokenization_params=tokenization_params, \n",
    "                              segmentation_params=segmentation_params,\n",
    "                              operation_space='sequence')\n",
    "segment = 'AATCAAGGAATTATTATCGTT'\n",
    "print(f'Segment sequence {segment}')\n",
    "tokens, kmers = tokenizer.tokenize(segment, all=True)\n",
    "print('Tokens:   ')\n",
    "for tokenset in tokens:\n",
    "       print(str(tokenset))\n",
    "print('___________________')\n",
    "         \n",
    "print('Kmers:   ')\n",
    "for tokenset in kmers:\n",
    "       print(str(tokenset))\n",
    "print('___________________')\n",
    "         \n",
    "         \n",
    "print('Restore the original string:')\n",
    "seq_toks = tokenizer.convert_ids_to_tokens(tokens[0])\n",
    "print('    ' + ''.join(seq_toks))\n",
    "\n",
    "s = pretty_print_overlapping_sequence(segment, kmers[0], tokenizer.tokenization_params)\n",
    "print(s)\n",
    "\n",
    "print('Checking, windows = 0')\n",
    "lca_kmers = tokenizer.tokenize(segment, lca_shift=0)\n",
    "print(str(lca_kmers))\n",
    "\n",
    "print('Checking, all == True')\n",
    "lca_kmers = tokenizer.tokenize(segment, all=True)\n",
    "print(str(lca_kmers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca924a-8307-4459-b0bf-eb74e778035c",
   "metadata": {},
   "source": [
    "## Checking for k-mer space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e12bb-316d-4c13-9049-a7736adcf561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915082f-85f0-4c5a-ae8e-9c03d1f5c553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277f3024-60e9-43d7-a588-1385d000ff9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['AATCAA']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AATCAA', 'GG', 'AA', 'TT', 'AT', 'TA', 'TC', 'GT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listát csinál belőle. De előbb nekünk ebből k-mer listák kellenének, ahogy az meg van írva\n",
    "segment = 'AATCAAGGAATTATTATCGTT'\n",
    "\n",
    "tokens, kmers = tokenizer.tokenize(segment, all=True)\n",
    "#tokenizer.id2token\n",
    "#tokenizer = ProkBERTTokenizer(operation_space='sequence')\n",
    "#tokenizer.id2token.get(10, 'NN')\n",
    "#tokenizer.id2token.get(10, 'NN')[-2:]\n",
    "#print(tokens[0])\n",
    "#print(kmers[0])\n",
    "#print(kmers[0])\n",
    "seq_toks = tokenizer.convert_ids_to_tokens(tokens[0])\n",
    "seq_toks\n",
    "# Nem tudja lefedni, akkor az uccsó az hiányozhat, mivel nem teljesen egyértelmű, hogy pontosan mi is van. \n",
    "#print(''.join(seq_toks))\n",
    "#print(segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2f223f-2268-4d5d-ad43-ec247768db1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 12:36:26,955 - INFO - Nr. line to cover the seq:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    AATCAAGGAATTATTATCGTT\n",
      "0.  AATCAA  AATTAT\n",
      "1.    TCAAGG  TTATTA\n",
      "2.      AAGGAA  ATTATC\n",
      "3.        GGAATT  TATCGT\n",
      "\n",
      "\n",
      "    AATCAAGGAATTATTATCGTT\n",
      "0.   ATCAAG  ATTATT\n",
      "1.     CAAGGA  TATTAT\n",
      "2.       AGGAAT  TTATCG\n",
      "3.         GAATTA  ATCGTT\n",
      "\n",
      "\n",
      "    AATCAAGGAATTATTATCGTT\n"
     ]
    }
   ],
   "source": [
    "#tokens[0]\n",
    "#kmers[0]\n",
    "#\n",
    "\n",
    "from sequtils import *\n",
    "\n",
    "s = pretty_print_overlapping_sequence(segment, kmers, tokenizer.tokenization_params)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f383bbf0-dbac-4e8d-a10f-e2b0b13a212c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [array([   2,    1,    1, 3117,  648, 2113,  980, 3320, 3899,    3],\n",
       "        dtype=uint16),\n",
       "  array([   2, 3441,    1,    1,    1, 3855,  165, 2580,  248, 3905,  978,\n",
       "         3296,    3], dtype=uint16)],\n",
       " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = ['AASTTAAGGAATTATTATCGT', 'TCCGTAASTTAAGGAATTATTATCGT']\n",
    "tokenizer.batch_encode_plus(seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8702658-b994-4899-99db-decf0791d4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AATCAA', 'GG', 'AA', 'TT', 'AT', 'TA', 'TC', 'GT']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06bfe1d-6082-4fdb-84fb-590834d2f621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['AATCAA']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AATCAA', 'GG', 'AA', 'TT', 'AT', 'TA', 'TC', 'GT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91dab7bf-f8a8-4a34-943e-f88340405162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 213, 3343, 165, 2580, 248, 3905, 978, 3296, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327348c-3c8a-4172-b623-53d429a01fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
