{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e13c256-83f6-4aa5-b900-b3e77f053711",
   "metadata": {},
   "source": [
    "## Szegmentálás és tokenizálás tesztelni egy példa fasta file-on, ami a ESKAPE pathogenekből szárzmazik, ez lehet a példa kód a későbbi feladatokra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "852184bd-f098-4606-98ec-e3b167b05739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 13:45:36,326 - INFO - Loading sequence data into memory!\n",
      "2023-08-10 13:45:36,327 - INFO - Since the fasta_files_list is a string, not list, we convert to a list.\n",
      "2023-08-10 13:45:36,329 - INFO - Checking input DataFrame!\n",
      "2023-08-10 13:45:36,329 - INFO - Checking input sequence_id is valid primary key in the DataFrame\n",
      "2023-08-10 13:45:36,330 - INFO - Sampling 19 segments from 3 sequences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SEQ_CONFIG_FILE environment variable has not been set. Using default value: /home/ligeti/github/prokbert/src/prokbert/configs/sequence_processing.yaml\n",
      "/home/ligeti/github/prokbert/src/prokbert\n",
      "/home/ligeti/github/prokbert/src/prokbert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cpu_cores_for_segmentation': 10,\n",
       " 'cpu_cores_for_tokenization': 48,\n",
       " 'batch_size_tokenization': 10000,\n",
       " 'batch_size_fasta_segmentation': 3}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "#from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "import yaml\n",
    "import pathlib\n",
    "from os.path import join\n",
    "import os\n",
    "import sys\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "current_path = str(pathlib.Path(os.getcwd()).parent)\n",
    "sys.path.append(current_path)\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('../)\n",
    "\n",
    "from config_utils import *\n",
    "from sequtils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "defconfig = SeqConfig()\n",
    "segmentation_params = defconfig.get_set_segmentation_parameters()\n",
    "tokenization_params = defconfig.get_and_set_tokenization_params()             \n",
    "comp_params = defconfig.get_set_computational_paramters()\n",
    "samplefasta_file = '/home/ligeti/github/prokbert/src/prokbert/data/sample_data/ESKAPE_sample.fasta'\n",
    "contigs = load_contigs(samplefasta_file,IsAddHeader=True,AsDataFrame=True, adding_reverse_complement=False)\n",
    "contigs['sequence_id'] = [0, 1, 2]\n",
    "sequences_ls = list(contigs['sequence'])\n",
    "\n",
    "act_segments = segment_sequence_contiguous(sequences_ls[0], segmentation_params, 1)\n",
    "segmentation_params['type'] = 'random'\n",
    "results = segment_sequences(contigs, segmentation_params, AsDataFrame=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d494fd10-71b1-49c9-9212-7ed5b137ccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 13:57:31,217 - INFO - Tokenization of a list of segments\n",
      "2023-08-10 13:57:31,426 - INFO - Tokenization of a list of segments\n"
     ]
    }
   ],
   "source": [
    "segments = list(results['segment'])\n",
    "segment_ids = list(results['segment_id'])\n",
    "\n",
    "num_cores = comp_params['cpu_cores_for_tokenization']\n",
    "batch_size = comp_params['batch_size_tokenization']\n",
    "\n",
    "r = batch_tokenize_segments_with_ids([segments, segment_ids], tokenization_params, num_cores, batch_size)\n",
    "r = batch_tokenize_segments_with_ids(results, tokenization_params, num_cores, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f87a09f-0f8d-4f17-95b2-715ce2badd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1116e-e7cc-44e8-a5c0-ad2ff83f2d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13883b-7767-49e2-98b5-980df44516a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85523ae-9648-4359-84be-17f00701d89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95184415-d9a8-4713-b166-b19337483880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c553c-fb52-4f4e-965b-573df908c54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d503a-ad94-49f3-86ba-c13d74c4e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03339843-9767-4c31-9e3d-5bc9150dab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sequence = [ str[0:50] for str in contigs['sequence']]\n",
    "test_contigs =contigs [ ['sequence_id', 'sequence']]\n",
    "test_contigs['sequence'] = n_sequence\n",
    "test_contigs.to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925792e6-9ee5-49b5-8805-3bc2b626913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = 'TCTTTGCTAAG'\n",
    "params_example = {\n",
    "            'shift': 1, \n",
    "            'max_segment_length': 512, \n",
    "            'max_unknown_token_proportion': 0.2, \n",
    "            'kmer': 5, \n",
    "            'token_limit': 10\n",
    "        }\n",
    "params_example = defconfig.get_and_set_tokenization_params(params_example)         \n",
    "\n",
    "result = lca_tokenize_segment(segment, params_example)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2842c94-5063-46c8-b3f1-b6dc765ad609",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_sequences(contigs, segmentation_params, AsDataFrame = True)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7037eb-5f6d-40d7-9f92-d7d77a63f0d3",
   "metadata": {},
   "source": [
    "## Adding the coverage based segmentation\n",
    "\n",
    "\n",
    "1. lépés valamilyen dataframe-t létrehozni, amiben\n",
    "cumsum, contig_lengths, sequences, contigs, ilyesmi. De nem tudom ezek között mi is a különbség\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1cf7c-7613-401d-a3d1-df8cdb6e97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = segmentation_params\n",
    "params['type'] = 'random'\n",
    "results = segment_sequences(contigs, params, AsDataFrame=True)\n",
    "results = segment_sequences(contigs, params, AsDataFrame=False)\n",
    "\n",
    "#pd.DataFrame(results)\n",
    "results = segment_sequences(contigs, params, AsDataFrame=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8133305-0546-44d1-a13a-ba3d10c2e6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0143c2d7-d8dc-4ab1-a797-1ccd30bcf42b",
   "metadata": {},
   "source": [
    "# Tokenizáló tesztelése\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b1da8-fcb3-44f1-80f2-0cb5a1110295",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_params = defconfig.get_and_set_tokenization_params()         \n",
    "tokenization_params['shift']=2\n",
    "tokenization_params['kmer']=5\n",
    "print(list(results['segment'])[0][0:30])\n",
    "\n",
    "tokens, kmers = lca_tokenize_segment(list(results['segment'])[0][0:30], tokenization_params)\n",
    "#tokens, kmers = lca_tokenize_segment('ATCTGT', tokenization_params)\n",
    "print(tokens)\n",
    "print(kmers)\n",
    "#tokens\n",
    "#print(kmers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48922202-898f-44ca-b75d-ec5df480482d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f14b2-cf35-42f5-bd3b-48f7a5bd0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_params['vocabmap']['[CLS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944364a4-c5b0-496b-b8e2-c4fabb75c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Specify the file path\n",
    "file_path = \"/home/ligeti/tokenizer_params.json\"\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(tokenization_params, json_file, indent=4)  # indent argument adds pretty formatting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55f1ca-af84-4a1d-8c09-a8aa91e89b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7abefd-ed56-4729-a023-561749cde60b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenization_params = defconfig.get_and_set_tokenization_params()\n",
    "segments = list(results['segment'])\n",
    "segment_ids = list(results['segment_id'])\n",
    "batch_tokenize_segments_with_ids(segments, segment_ids, tokenization_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51a4c3-be89-4dc2-89b4-44e392b00072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0d455-8386-424c-afb2-989e82231bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
